/*******************************************************************************
*   ___   public
*  ¦OUX¦  C+
*  ¦/C+¦  OUX/C+ OS
*   ---   UEFI boot loader
*         simple common procedures
* ©overcq                on ‟Gentoo Linux 23.0” “x86_64”              2025‒4‒6 c
*******************************************************************************/
_inline
N
E_asm_I_bsf( N n
){  N i;
        #if 0 // defined( __i386__ ) || defined( __x86_64__ )
    N v;
    __asm__ (
            #if defined( __i386__ )
    "\n" "movl  $~0,%0"
            #else
    "\n" "movq  $~0,%0"
            #endif
    "\n" "bsf   %2,%1"
    "\n" "cmove %0,%1"
    : "=rm" (v), "=r" (i)
    : "rm" (n)
    : "cc"
    );
        #else
    if(n)
        i = __builtin_ctzl(n);
    else
        i = ~0;
        #endif
    return i;
}
_inline
N
E_asm_I_bsr( N n
){  N i;
        #if 0 //defined( __i386__ ) || defined( __x86_64__ )
    N v;
    __asm__ (
            #if defined( __i386__ )
    "\n" "movl  $~0,%0"
            #else
    "\n" "movq  $~0,%0"
            #endif
    "\n" "bsr   %2,%1"
    "\n" "cmove %0,%1"
    : "=rm" (v), "=r" (i)
    : "rm" (n)
    : "cc"
    );
        #else
    if(n)
        i = sizeof(N) * 8 - 1 - __builtin_clzl(n);
    else
        i = ~0;
        #endif
    return i;
}
//==============================================================================
_inline
B
E_simple_T_add_overflow(
  N a
, N b
){  return a + b < a;
}
_inline
B
E_simple_T_multiply_overflow(
  N a
, N b
){  return a && b
    && ( E_asm_I_bsr(a) != ~0ULL ? E_asm_I_bsr(a) : 0 )
      + ( E_asm_I_bsr(b) != ~0ULL ? E_asm_I_bsr(b) : 0 )
      >= sizeof(N) * 8;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_inline
B
E_simple_Z_n_T_power_2( N n
){  return n ? ( _v( n, 1 ) << E_asm_I_bsf(n) ) == n : no;
}
_inline
B
E_simple_Z_n_T_aligned_to_v2( N n
, N v2
){  return !( n & ( v2 - 1 ));
}
//------------------------------------------------------------------------------
_inline
N
E_simple_Z_n_I_mod_i2( N n
, N i
){  return n & ( _v( n, ~0 ) >> ( sizeof(n) * 8 - i ));
}
_inline
N
E_simple_Z_n_I_align_down_to_i2( N n
, N i
){  return n & ( _v( n, ~0 ) << i );
}
_inline
N
E_simple_Z_n_I_align_up_to_i2( N n
, N i
){  N a = _v( n, ~0 ) << i;
    return ( n + ~a ) & a;
}
_inline
N
E_simple_Z_n_I_align_down_to_v2( N n
, N v2
){  if( !v2 )
        return n;
    return n & ~( v2 - 1 );
}
_inline
N
E_simple_Z_n_I_align_up_to_v2( N n
, N v2
){  if( !v2 )
        return n;
    N a = v2 - 1;
    return ( n + a ) & ~a;
}
_inline
N
E_simple_Z_n_I_align_down_to_v( N n
, N v
){  return n - ( n % v );
}
_inline
N
E_simple_Z_n_I_align_up_to_v( N n
, N v
){  N a = n % v;
    return a ? n + v - a : n;
}
_inline
N
E_simple_Z_n_I_align_up_to_first_i2( N n
){  return n ? ( _v( n, 1 ) << E_simple_Z_n_I_align_down_to_i2( n, E_asm_I_bsf(n) )) : n;
}
_inline
N
E_simple_Z_n_I_align_down_to_u_R_count( N n
, N u
){  return n / u;
}
_inline
N
E_simple_Z_n_I_align_up_to_u_R_count( N n
, N u
){  N a = n / u;
    if( n % u )
        a += u;
    return a;
}
_inline
N
E_simple_Z_n_I_align_down( N n
){  if( !n )
        return n;
    return _v( n, 1 ) << ( sizeof(N) * 8 - 1 - __builtin_clzl(n) );
}
_inline
N
E_simple_Z_n_I_align_up( N n
){  if( !n )
        return n;
    N a = _v( n, ~0 ) << ( sizeof(N) * 8 - 1 - __builtin_clzl(n) );
    return ( n + ~a ) & a;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_inline
B
E_simple_Z_p_T_inside( P p_1
, P p_2
, N l_2
){  return (Pc)p_1 >= (Pc)p_2
    && (Pc)p_1 < (Pc)p_2 + l_2;
}
_inline
B
E_simple_Z_p_T_cross( P p_1
, N l_1
, P p_2
, N l_2
){  return E_simple_Z_p_T_inside( p_2, p_1, l_1 )
    || E_simple_Z_p_T_inside( p_1, p_2, l_2 );
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#define E_simple_Z_p_T_aligned_to_v2(p,v2)      E_simple_Z_n_T_aligned_to_v2( (N)p, v2 )
//------------------------------------------------------------------------------
#define E_simple_Z_p_I_align_down_to_i2(p,i)    (P)E_simple_Z_n_I_align_down_to_i2( (N)p, i )
#define E_simple_Z_p_I_align_up_to_i2(p,i)      (P)E_simple_Z_n_I_align_up_to_i2( (N)p, i )
#define E_simple_Z_p_I_align_down_to_v2(p,v2)   (P)E_simple_Z_n_I_align_down_to_v2( (N)p, v2 )
#define E_simple_Z_p_I_align_up_to_v2(p,v2)     (P)E_simple_Z_n_I_align_up_to_v2( (N)p, v2 )
#define E_simple_Z_p_I_align_down(p)            (P)E_simple_Z_n_I_align_down( (N)p )
#define E_simple_Z_p_I_align_up(p)              (P)E_simple_Z_n_I_align_up( (N)p )
//==============================================================================
_inline
Pc
E_text_Z_s0_R_end( Pc s
){  while( *s )
        s++;
    return s;
}
_inline
N
E_text_Z_s0_R_l( Pc s
){  return E_text_Z_s0_R_end(s) - s;
}
_inline
Pc
E_text_Z_s0_R_end_0( Pc s
){  return E_text_Z_s0_R_end(s) + 1;
}
_inline
N
E_text_Z_s0_R_l_0( Pc s
){  return E_text_Z_s0_R_end_0(s) - s;
}
//------------------------------------------------------------------------------
_inline
Pc
E_text_Z_s_P_0( Pc s
){  *s++ = '\0';
    return s;
}
Pc E_text_Z_s_R_search_last_c( Pc, Pc, C );
_inline
Pc
E_text_Z_s0_R_search_last_c( Pc s
, C c
){  return E_text_Z_s_R_search_last_c( s, E_text_Z_s0_R_end_0(s), c );
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_inline
B
E_mem_Q_tab_T( struct E_mem_Q_tab_Z *tab_
, I id
){  return (In)id < (In)tab_->index_n
    && tab_->index[id];
}
_inline
I
E_mem_Q_tab_R_n( struct E_mem_Q_tab_Z *tab_
){  return tab_->data_n;
}
_inline
P
E_mem_Q_tab_R( struct E_mem_Q_tab_Z *tab_
, I id
){  return tab_->index[id];
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_inline
void
E_flow_I_lock( B *lock
){  __asm__ volatile (
    "\n"    "mov    $1,%%cl"
    "\n0:"  "xor    %%al,%%al"
    "\n"    "lock cmpxchg %%cl,%0"
    "\n"    "je     0f"
    "\n"    "pause"
    "\n"    "jmp    0b"
    "\n0:"
    : "+m" ( *lock )
    :
    : "cc", "al", "cl"
    );
}
_inline
void
E_flow_I_unlock( B *lock
){  *lock = no;
}
_inline
N
E_flow_I_lock_r( B *lock
, N *r
){  E_flow_I_lock(lock);
    return *r;
}
_inline
void
E_flow_I_unlock_r( B *lock
, N *r
, N r_
){  E_flow_I_unlock(lock);
    __asm__ volatile (
    "\n"    "lock xchg %1,%0"
    : "+m" ( *r ), "+r" ( r_ )
    );
}
/******************************************************************************/
